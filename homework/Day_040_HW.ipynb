{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "使用 Sklearn 中的 Lasso, Ridge 模型，來訓練各種資料集，務必了解送進去模型訓練的**資料型態**為何，也請了解模型中各項參數的意義。\n",
    "\n",
    "機器學習的模型非常多種，但要訓練的資料多半有固定的格式，確保你了解訓練資料的格式為何，這樣在應用新模型時，就能夠最快的上手開始訓練！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習時間\n",
    "試著使用 sklearn datasets 的其他資料集 (boston, ...)，來訓練自己的線性迴歸模型，並加上適當的正則話來觀察訓練情形。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[  6.32000000e-03,   1.80000000e+01,   2.31000000e+00, ...,\n",
      "          1.53000000e+01,   3.96900000e+02,   4.98000000e+00],\n",
      "       [  2.73100000e-02,   0.00000000e+00,   7.07000000e+00, ...,\n",
      "          1.78000000e+01,   3.96900000e+02,   9.14000000e+00],\n",
      "       [  2.72900000e-02,   0.00000000e+00,   7.07000000e+00, ...,\n",
      "          1.78000000e+01,   3.92830000e+02,   4.03000000e+00],\n",
      "       ..., \n",
      "       [  6.07600000e-02,   0.00000000e+00,   1.19300000e+01, ...,\n",
      "          2.10000000e+01,   3.96900000e+02,   5.64000000e+00],\n",
      "       [  1.09590000e-01,   0.00000000e+00,   1.19300000e+01, ...,\n",
      "          2.10000000e+01,   3.93450000e+02,   6.48000000e+00],\n",
      "       [  4.74100000e-02,   0.00000000e+00,   1.19300000e+01, ...,\n",
      "          2.10000000e+01,   3.96900000e+02,   7.88000000e+00]]), 'target': array([ 24. ,  21.6,  34.7,  33.4,  36.2,  28.7,  22.9,  27.1,  16.5,\n",
      "        18.9,  15. ,  18.9,  21.7,  20.4,  18.2,  19.9,  23.1,  17.5,\n",
      "        20.2,  18.2,  13.6,  19.6,  15.2,  14.5,  15.6,  13.9,  16.6,\n",
      "        14.8,  18.4,  21. ,  12.7,  14.5,  13.2,  13.1,  13.5,  18.9,\n",
      "        20. ,  21. ,  24.7,  30.8,  34.9,  26.6,  25.3,  24.7,  21.2,\n",
      "        19.3,  20. ,  16.6,  14.4,  19.4,  19.7,  20.5,  25. ,  23.4,\n",
      "        18.9,  35.4,  24.7,  31.6,  23.3,  19.6,  18.7,  16. ,  22.2,\n",
      "        25. ,  33. ,  23.5,  19.4,  22. ,  17.4,  20.9,  24.2,  21.7,\n",
      "        22.8,  23.4,  24.1,  21.4,  20. ,  20.8,  21.2,  20.3,  28. ,\n",
      "        23.9,  24.8,  22.9,  23.9,  26.6,  22.5,  22.2,  23.6,  28.7,\n",
      "        22.6,  22. ,  22.9,  25. ,  20.6,  28.4,  21.4,  38.7,  43.8,\n",
      "        33.2,  27.5,  26.5,  18.6,  19.3,  20.1,  19.5,  19.5,  20.4,\n",
      "        19.8,  19.4,  21.7,  22.8,  18.8,  18.7,  18.5,  18.3,  21.2,\n",
      "        19.2,  20.4,  19.3,  22. ,  20.3,  20.5,  17.3,  18.8,  21.4,\n",
      "        15.7,  16.2,  18. ,  14.3,  19.2,  19.6,  23. ,  18.4,  15.6,\n",
      "        18.1,  17.4,  17.1,  13.3,  17.8,  14. ,  14.4,  13.4,  15.6,\n",
      "        11.8,  13.8,  15.6,  14.6,  17.8,  15.4,  21.5,  19.6,  15.3,\n",
      "        19.4,  17. ,  15.6,  13.1,  41.3,  24.3,  23.3,  27. ,  50. ,\n",
      "        50. ,  50. ,  22.7,  25. ,  50. ,  23.8,  23.8,  22.3,  17.4,\n",
      "        19.1,  23.1,  23.6,  22.6,  29.4,  23.2,  24.6,  29.9,  37.2,\n",
      "        39.8,  36.2,  37.9,  32.5,  26.4,  29.6,  50. ,  32. ,  29.8,\n",
      "        34.9,  37. ,  30.5,  36.4,  31.1,  29.1,  50. ,  33.3,  30.3,\n",
      "        34.6,  34.9,  32.9,  24.1,  42.3,  48.5,  50. ,  22.6,  24.4,\n",
      "        22.5,  24.4,  20. ,  21.7,  19.3,  22.4,  28.1,  23.7,  25. ,\n",
      "        23.3,  28.7,  21.5,  23. ,  26.7,  21.7,  27.5,  30.1,  44.8,\n",
      "        50. ,  37.6,  31.6,  46.7,  31.5,  24.3,  31.7,  41.7,  48.3,\n",
      "        29. ,  24. ,  25.1,  31.5,  23.7,  23.3,  22. ,  20.1,  22.2,\n",
      "        23.7,  17.6,  18.5,  24.3,  20.5,  24.5,  26.2,  24.4,  24.8,\n",
      "        29.6,  42.8,  21.9,  20.9,  44. ,  50. ,  36. ,  30.1,  33.8,\n",
      "        43.1,  48.8,  31. ,  36.5,  22.8,  30.7,  50. ,  43.5,  20.7,\n",
      "        21.1,  25.2,  24.4,  35.2,  32.4,  32. ,  33.2,  33.1,  29.1,\n",
      "        35.1,  45.4,  35.4,  46. ,  50. ,  32.2,  22. ,  20.1,  23.2,\n",
      "        22.3,  24.8,  28.5,  37.3,  27.9,  23.9,  21.7,  28.6,  27.1,\n",
      "        20.3,  22.5,  29. ,  24.8,  22. ,  26.4,  33.1,  36.1,  28.4,\n",
      "        33.4,  28.2,  22.8,  20.3,  16.1,  22.1,  19.4,  21.6,  23.8,\n",
      "        16.2,  17.8,  19.8,  23.1,  21. ,  23.8,  23.1,  20.4,  18.5,\n",
      "        25. ,  24.6,  23. ,  22.2,  19.3,  22.6,  19.8,  17.1,  19.4,\n",
      "        22.2,  20.7,  21.1,  19.5,  18.5,  20.6,  19. ,  18.7,  32.7,\n",
      "        16.5,  23.9,  31.2,  17.5,  17.2,  23.1,  24.5,  26.6,  22.9,\n",
      "        24.1,  18.6,  30.1,  18.2,  20.6,  17.8,  21.7,  22.7,  22.6,\n",
      "        25. ,  19.9,  20.8,  16.8,  21.9,  27.5,  21.9,  23.1,  50. ,\n",
      "        50. ,  50. ,  50. ,  50. ,  13.8,  13.8,  15. ,  13.9,  13.3,\n",
      "        13.1,  10.2,  10.4,  10.9,  11.3,  12.3,   8.8,   7.2,  10.5,\n",
      "         7.4,  10.2,  11.5,  15.1,  23.2,   9.7,  13.8,  12.7,  13.1,\n",
      "        12.5,   8.5,   5. ,   6.3,   5.6,   7.2,  12.1,   8.3,   8.5,\n",
      "         5. ,  11.9,  27.9,  17.2,  27.5,  15. ,  17.2,  17.9,  16.3,\n",
      "         7. ,   7.2,   7.5,  10.4,   8.8,   8.4,  16.7,  14.2,  20.8,\n",
      "        13.4,  11.7,   8.3,  10.2,  10.9,  11. ,   9.5,  14.5,  14.1,\n",
      "        16.1,  14.3,  11.7,  13.4,   9.6,   8.7,   8.4,  12.8,  10.5,\n",
      "        17.1,  18.4,  15.4,  10.8,  11.8,  14.9,  12.6,  14.1,  13. ,\n",
      "        13.4,  15.2,  16.1,  17.8,  14.9,  14.1,  12.7,  13.5,  14.9,\n",
      "        20. ,  16.4,  17.7,  19.5,  20.2,  21.4,  19.9,  19. ,  19.1,\n",
      "        19.1,  20.1,  19.9,  19.6,  23.2,  29.8,  13.8,  13.3,  16.7,\n",
      "        12. ,  14.6,  21.4,  23. ,  23.7,  25. ,  21.8,  20.6,  21.2,\n",
      "        19.1,  20.6,  15.2,   7. ,   8.1,  13.6,  20.1,  21.8,  24.5,\n",
      "        23.1,  19.7,  18.3,  21.2,  17.5,  16.8,  22.4,  20.6,  23.9,\n",
      "        22. ,  11.9]), 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
      "       'TAX', 'PTRATIO', 'B', 'LSTAT'],\n",
      "      dtype='<U7'), 'DESCR': \"Boston House Prices dataset\\n===========================\\n\\nNotes\\n------\\nData Set Characteristics:  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive\\n    \\n    :Median Value (attribute 14) is usually the target\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttp://archive.ics.uci.edu/ml/datasets/Housing\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n**References**\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\\n\"}\n",
      "[[  6.32000000e-03   1.80000000e+01   2.31000000e+00 ...,   1.53000000e+01\n",
      "    3.96900000e+02   4.98000000e+00]\n",
      " [  2.73100000e-02   0.00000000e+00   7.07000000e+00 ...,   1.78000000e+01\n",
      "    3.96900000e+02   9.14000000e+00]\n",
      " [  2.72900000e-02   0.00000000e+00   7.07000000e+00 ...,   1.78000000e+01\n",
      "    3.92830000e+02   4.03000000e+00]\n",
      " ..., \n",
      " [  6.07600000e-02   0.00000000e+00   1.19300000e+01 ...,   2.10000000e+01\n",
      "    3.96900000e+02   5.64000000e+00]\n",
      " [  1.09590000e-01   0.00000000e+00   1.19300000e+01 ...,   2.10000000e+01\n",
      "    3.93450000e+02   6.48000000e+00]\n",
      " [  4.74100000e-02   0.00000000e+00   1.19300000e+01 ...,   2.10000000e+01\n",
      "    3.96900000e+02   7.88000000e+00]]\n",
      "[ 24.   21.6  34.7  33.4  36.2  28.7  22.9  27.1  16.5  18.9  15.   18.9\n",
      "  21.7  20.4  18.2  19.9  23.1  17.5  20.2  18.2  13.6  19.6  15.2  14.5\n",
      "  15.6  13.9  16.6  14.8  18.4  21.   12.7  14.5  13.2  13.1  13.5  18.9\n",
      "  20.   21.   24.7  30.8  34.9  26.6  25.3  24.7  21.2  19.3  20.   16.6\n",
      "  14.4  19.4  19.7  20.5  25.   23.4  18.9  35.4  24.7  31.6  23.3  19.6\n",
      "  18.7  16.   22.2  25.   33.   23.5  19.4  22.   17.4  20.9  24.2  21.7\n",
      "  22.8  23.4  24.1  21.4  20.   20.8  21.2  20.3  28.   23.9  24.8  22.9\n",
      "  23.9  26.6  22.5  22.2  23.6  28.7  22.6  22.   22.9  25.   20.6  28.4\n",
      "  21.4  38.7  43.8  33.2  27.5  26.5  18.6  19.3  20.1  19.5  19.5  20.4\n",
      "  19.8  19.4  21.7  22.8  18.8  18.7  18.5  18.3  21.2  19.2  20.4  19.3\n",
      "  22.   20.3  20.5  17.3  18.8  21.4  15.7  16.2  18.   14.3  19.2  19.6\n",
      "  23.   18.4  15.6  18.1  17.4  17.1  13.3  17.8  14.   14.4  13.4  15.6\n",
      "  11.8  13.8  15.6  14.6  17.8  15.4  21.5  19.6  15.3  19.4  17.   15.6\n",
      "  13.1  41.3  24.3  23.3  27.   50.   50.   50.   22.7  25.   50.   23.8\n",
      "  23.8  22.3  17.4  19.1  23.1  23.6  22.6  29.4  23.2  24.6  29.9  37.2\n",
      "  39.8  36.2  37.9  32.5  26.4  29.6  50.   32.   29.8  34.9  37.   30.5\n",
      "  36.4  31.1  29.1  50.   33.3  30.3  34.6  34.9  32.9  24.1  42.3  48.5\n",
      "  50.   22.6  24.4  22.5  24.4  20.   21.7  19.3  22.4  28.1  23.7  25.\n",
      "  23.3  28.7  21.5  23.   26.7  21.7  27.5  30.1  44.8  50.   37.6  31.6\n",
      "  46.7  31.5  24.3  31.7  41.7  48.3  29.   24.   25.1  31.5  23.7  23.3\n",
      "  22.   20.1  22.2  23.7  17.6  18.5  24.3  20.5  24.5  26.2  24.4  24.8\n",
      "  29.6  42.8  21.9  20.9  44.   50.   36.   30.1  33.8  43.1  48.8  31.\n",
      "  36.5  22.8  30.7  50.   43.5  20.7  21.1  25.2  24.4  35.2  32.4  32.\n",
      "  33.2  33.1  29.1  35.1  45.4  35.4  46.   50.   32.2  22.   20.1  23.2\n",
      "  22.3  24.8  28.5  37.3  27.9  23.9  21.7  28.6  27.1  20.3  22.5  29.\n",
      "  24.8  22.   26.4  33.1  36.1  28.4  33.4  28.2  22.8  20.3  16.1  22.1\n",
      "  19.4  21.6  23.8  16.2  17.8  19.8  23.1  21.   23.8  23.1  20.4  18.5\n",
      "  25.   24.6  23.   22.2  19.3  22.6  19.8  17.1  19.4  22.2  20.7  21.1\n",
      "  19.5  18.5  20.6  19.   18.7  32.7  16.5  23.9  31.2  17.5  17.2  23.1\n",
      "  24.5  26.6  22.9  24.1  18.6  30.1  18.2  20.6  17.8  21.7  22.7  22.6\n",
      "  25.   19.9  20.8  16.8  21.9  27.5  21.9  23.1  50.   50.   50.   50.\n",
      "  50.   13.8  13.8  15.   13.9  13.3  13.1  10.2  10.4  10.9  11.3  12.3\n",
      "   8.8   7.2  10.5   7.4  10.2  11.5  15.1  23.2   9.7  13.8  12.7  13.1\n",
      "  12.5   8.5   5.    6.3   5.6   7.2  12.1   8.3   8.5   5.   11.9  27.9\n",
      "  17.2  27.5  15.   17.2  17.9  16.3   7.    7.2   7.5  10.4   8.8   8.4\n",
      "  16.7  14.2  20.8  13.4  11.7   8.3  10.2  10.9  11.    9.5  14.5  14.1\n",
      "  16.1  14.3  11.7  13.4   9.6   8.7   8.4  12.8  10.5  17.1  18.4  15.4\n",
      "  10.8  11.8  14.9  12.6  14.1  13.   13.4  15.2  16.1  17.8  14.9  14.1\n",
      "  12.7  13.5  14.9  20.   16.4  17.7  19.5  20.2  21.4  19.9  19.   19.1\n",
      "  19.1  20.1  19.9  19.6  23.2  29.8  13.8  13.3  16.7  12.   14.6  21.4\n",
      "  23.   23.7  25.   21.8  20.6  21.2  19.1  20.6  15.2   7.    8.1  13.6\n",
      "  20.1  21.8  24.5  23.1  19.7  18.3  21.2  17.5  16.8  22.4  20.6  23.9\n",
      "  22.   11.9]\n",
      "[ 12.06508881  26.98544801  17.59242607  18.15842166  36.91656975\n",
      "  25.43573299  31.08610267  19.7198111   19.63707329  22.95805783\n",
      "  28.38566642  28.49200286  19.00260488  32.42578552  21.52409329\n",
      "  15.20850151  21.23763462  11.60093062  11.38018796  13.62527521\n",
      "   5.65831897  17.34095216  20.81305124  22.51181212  16.39301984\n",
      "  20.32279223  17.89495414  14.23685883  21.08606151  17.50432715\n",
      "  14.50006271  23.63998403  34.31768468  22.23613308  16.82042372\n",
      "  20.15196012  30.67786122  35.60991076  23.50419347  24.67227487\n",
      "  36.91121083  32.34217047  19.10793629  32.1983094   33.42967099\n",
      "  25.53182714  40.63022496  18.21661123  19.33922512  23.80347638\n",
      "  33.42145032  26.14932391  18.09348831  28.20740859  13.36375701\n",
      "  23.34559702  24.44787362  33.55417247  16.70358813  36.57198614\n",
      "  15.69970374  18.53837398  32.14966264  15.50497853  39.02306656\n",
      "  27.3802001   31.96945412  10.04948066  19.12888926  21.72507855\n",
      "  23.14985491  22.82968181  22.50573116  28.2154115   17.12753638\n",
      "  23.0769054   16.65810264  25.18167994  13.68123205  19.78570713\n",
      "  22.31929876  20.23599944  28.36140561  19.12682135  30.49416458\n",
      "  22.2628809   29.98336484  19.27691266  23.74333026  38.32047363\n",
      "  31.24834261  41.91630239  18.61520467  37.46825552  19.65956084\n",
      "  23.45085031  26.55883415  22.39913073   9.58873937  20.39482401\n",
      "   9.18010124  27.36961983]\n"
     ]
    }
   ],
   "source": [
    "boston = datasets.load_boston()\n",
    "\n",
    "# 切分訓練集/測試集\n",
    "x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2, random_state=4)\n",
    "print(boston)\n",
    "print(boston.data)\n",
    "print(boston.target)\n",
    "# 建立一個線性回歸模型\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# 將訓練資料丟進去模型訓練\n",
    "regr.fit(x_train, y_train)\n",
    "\n",
    "# 將測試資料丟進模型得到預測結果\n",
    "y_pred = regr.predict(x_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.14743504e-01   4.70875035e-02   8.70282354e-03   3.23818824e+00\n",
      "  -1.67240567e+01   3.87662996e+00  -1.08218769e-02  -1.54144627e+00\n",
      "   2.92604151e-01  -1.33989537e-02  -9.07306805e-01   8.91271054e-03\n",
      "  -4.58747039e-01]\n"
     ]
    }
   ],
   "source": [
    "print(regr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 25.41\n"
     ]
    }
   ],
   "source": [
    "# 預測值與實際值的差距，使用 MSE\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 讀取糖尿病資料集\n",
    "boston = datasets.load_boston()\n",
    "\n",
    "# 切分訓練集/測試集\n",
    "x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2, random_state=4)\n",
    "\n",
    "# 建立一個線性回歸模型\n",
    "lasso = linear_model.Lasso(alpha=0.5)\n",
    "\n",
    "# 將訓練資料丟進去模型訓練\n",
    "lasso.fit(x_train, y_train)\n",
    "\n",
    "# 將測試資料丟進模型得到預測結果\n",
    "y_pred = lasso.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08764892,  0.04826558, -0.01084097,  0.        , -0.        ,\n",
       "        2.6557073 , -0.00304648, -0.98409614,  0.2561194 , -0.01593011,\n",
       "       -0.73302892,  0.00892585, -0.59291607])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 印出各特徵對應的係數，可以看到許多係數都變成 0，Lasso Regression 的確可以做特徵選取\n",
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 26.94\n"
     ]
    }
   ],
   "source": [
    "# 預測值與實際值的差距，使用 MSE\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 11.95137587  26.92926317  17.50754401  17.90777081  36.84622811\n",
      "  25.37205816  31.19563659  19.56450753  19.42763884  23.31770129\n",
      "  28.5233485   28.43971042  19.03006849  32.21679147  21.58070932\n",
      "  15.28523252  21.21147973  11.62550402  11.16340174  13.67904743\n",
      "   5.67877757  17.74846771  20.67957497  22.44553578  16.43838836\n",
      "  20.27690737  17.62929393  14.23930838  20.916194    17.41459019\n",
      "  14.50440291  23.64463912  34.43372422  22.17297662  16.86682402\n",
      "  20.12289626  30.68236006  35.67763278  23.52776506  24.60347581\n",
      "  36.90486051  32.21499342  19.20103433  32.17533609  33.24352262\n",
      "  25.426046    40.6005286   18.07426869  19.4889773   23.76905434\n",
      "  33.4079965   26.01146518  18.11406002  28.0734415   13.36124586\n",
      "  23.27714334  24.43963432  33.50128282  16.84325531  36.45811732\n",
      "  15.7268167   18.77655649  32.04970168  15.39412731  39.10902731\n",
      "  27.48685838  31.81471464   9.99680407  19.00936522  21.68884934\n",
      "  23.15608523  22.81654749  22.60312486  28.13917773  16.96942731\n",
      "  23.15670252  16.65218461  25.18293022  13.72535385  19.72089436\n",
      "  22.27780377  20.11449682  28.35087873  19.2151702   30.44212266\n",
      "  22.36270718  30.01032812  19.35339847  23.58911867  38.20477658\n",
      "  31.28755283  41.84326037  18.63258887  37.45947478  19.72775326\n",
      "  23.44480978  26.40328231  22.27835275   9.95671506  20.54973627\n",
      "   9.26456126  27.24833569]\n"
     ]
    }
   ],
   "source": [
    "# 讀取糖尿病資料集\n",
    "boston = datasets.load_boston()\n",
    "\n",
    "# 切分訓練集/測試集\n",
    "x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2, random_state=4)\n",
    "\n",
    "# 建立一個線性回歸模型\n",
    "ridge = linear_model.Ridge(alpha=0.2)\n",
    "\n",
    "# 將訓練資料丟進去模型訓練\n",
    "ridge.fit(x_train, y_train)\n",
    "\n",
    "# 將測試資料丟進模型得到預測結果\n",
    "y_pred1 = ridge.predict(x_test)\n",
    "print(y_pred1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.13594812e-01   4.73261361e-02  -1.35251700e-03   3.16637540e+00\n",
      "  -1.44524864e+01   3.89458731e+00  -1.28672334e-02  -1.50809881e+00\n",
      "   2.86845788e-01  -1.35414397e-02  -8.83752369e-01   9.01959298e-03\n",
      "  -4.61146953e-01]\n"
     ]
    }
   ],
   "source": [
    "# 印出 Ridge 的參數，可以很明顯看到比起 Linear Regression，參數的數值都明顯小了許多\n",
    "print(ridge.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 25.48\n"
     ]
    }
   ],
   "source": [
    "# 預測值與實際值的差距，使用 MSE\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#L1, and L2 alpha越大,MSE越差,可能是資料沒有Over-fitting 的情況，是可以不需要一開始就加上太強的正規化的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
