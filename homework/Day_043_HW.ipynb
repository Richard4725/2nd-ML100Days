{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "了解隨機森林改善了決策樹的什麼缺點？是用什麼方法改進的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業\n",
    "\n",
    "閱讀以下兩篇文獻，了解隨機森林原理，並試著回答後續的思考問題\n",
    "- [隨機森林 (random forest) - 中文](http://hhtucode.blogspot.tw/2013/06/ml-random-forest.html)\n",
    "- [how random forest works - 英文](https://medium.com/@Synced/how-random-forest-algorithm-works-in-machine-learning-3c0fe15b6674)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. 隨機森林中的每一棵樹，是希望能夠\n",
    "\n",
    "    - 沒有任何限制，讓樹可以持續生長 (讓樹生成很深，讓模型變得複雜)\n",
    "    \n",
    "    - 不要過度生長，避免 Overfitting\n",
    "    \n",
    "    \n",
    "2. 假設總共有 N 筆資料，每棵樹用取後放回的方式抽了總共 N 筆資料生成，請問這棵樹大約使用了多少 % 不重複的原資料生成?\n",
    "hint: 0.632 bootstrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1.假設總共有 N 筆資料，每棵樹用取後放回的方式抽了總共 N 筆資料生成，請問這棵樹大約使用了多少 % 不重複的原資料生成? hint: 0.632 bootstrap\n",
    "ANS:\n",
    "\n",
    "    http://www.tool188.com/tutorial/ml-random-forest.html\n",
    "        \n",
    "    问题描述\n",
    "一个盒子里有100个小球（编号1到100），每次从盒子里随机挑选一个小球，记录该球的编号并将小球放回。重复抽样步骤100次，\n",
    "问抽样得到的不重复小球的个数是多少？\n",
    "\n",
    "问题分析\n",
    "首先，问题的答案应该是一个概率意义上的值。\n",
    "\n",
    "考虑一个特定的小球A，每次抽样A被抽到的概率为1/100，A没有被抽到的概率为1 － 1/100，则经过100次抽样，A没有被抽到的概率 \n",
    "P ＝(1 － 1/100) ^ 100。\n",
    "\n",
    "当样本个数不是100，而是非常大的数的时候（比如为x，x非常大），A没有被抽到的概率 P＝(1 - 1/x) ^ x。这个式子和我们熟知的\n",
    "一个公式非常像：(1 + 1/x) ^ x ＝ e （x取正无穷）。\n",
    "\n",
    "设 P ＝ (1 - 1 / x) ^ x，则 1/P = ( (1 + 1/(x-1) ) ^ (x -1) ) * ( 1 + 1/(x -1))，即 P＝1/e ＝ 0.368，解释为在每\n",
    "一次抽样中，每一个小球不被抽到大概率为0.368，经过100次抽样，约有100 ＊ (1 - P) = 63个不重复大小球会被抽到。\n",
    "\n",
    "2.了解隨機森林改善了決策樹的什麼缺點？是用什麼方法改進的？\n",
    "\n",
    "ANS:\n",
    "\n",
    "https://medium.com/@Synced/how-random-forest-algorithm-works-in-machine-learning-3c0fe15b6674\n",
    "    \n",
    "The difference between Random Forest algorithm and the decision tree algorithm is that in Random Forest,\n",
    "the process es of finding the root node and splitting the feature nodes will run randomly.\n",
    "\n",
    "    Advantages of Random Forest algorithm.\n",
    "Compared with other classification techniques, there are three advantages as the author mentioned.\n",
    "\n",
    "For applications in classification problems, Random Forest algorithm will avoid the overfitting problem\n",
    "For both classification and regression task, the same random forest algorithm can be used\n",
    "The Random Forest algorithm can be used for identifying the most important features from the training dataset, \n",
    "in other words, feature engineering.\n",
    "\n",
    "\n",
    "http://hhtucode.blogspot.com/2013/06/ml-random-forest.html\n",
    "    \n",
    "因為 Random Forest 我從來沒有在 ML 課學過, 完全是自學的\n",
    "\n",
    "那來把最上述的一句話我稍微展開一點:\n",
    "\n",
    "在 training data 中, 從中取出一些 feature & 部份 data 產生出 Tree (通常是CART)\n",
    "\n",
    "並且重複這步驟多次, 會產生出多棵 Tree 來\n",
    "\n",
    "最後利用 Ensemble (Majority Vote) 的方法, 結合所有 Tree, 就完成了 Random Forest\n",
    "\n",
    "首先故事是這樣的 Random Forest 其實是靠底下的 Tree 建立而成的\n",
    "\n",
    "而這些 Tree 的差異就是在組成所用的 data 跟 feature 都有點不一樣\n",
    "\n",
    "所以我們先從準備 training data 開始\n",
    "\n",
    "Bootstrap\n",
    "\n",
    "首先要建立一棵 Tree, 不會用全部的 training data 去 training 一棵 Tree\n",
    "\n",
    "我們為甚麼不用全部的 training data 餵進去就好?\n",
    "\n",
    "如果全部都餵的話, 那我不管建立幾棵 Tree, 結果都是一樣的, 那我乾脆就只要一棵就好了~\n",
    "\n",
    "所以為了讓每棵有所不同, 主要就是 training data 的採樣結果都會不太一樣\n",
    "\n",
    "這邊採樣的方式就是 Bagging\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
